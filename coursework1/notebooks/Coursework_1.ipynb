{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1\n",
    "\n",
    "This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the MLP2024_25_CW1_Spec.pdf (see Learn,  Assignment Submission, Coursework 1). The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from mlp.layers import AffineLayer, ReluLayer, DropoutLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "from mlp.penalties import L1Penalty, L2Penalty\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/path/to/mlpractical')\n",
    "from mlp.data_providers import EMNISTDataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Seed a random number generator\n",
    "        seed = 11102019 \n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.batch_size = 100\n",
    "        # Set up a logger object to print info about the training run to stdout\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "    def train_model_and_plot_stats(self,\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "        # As well as monitoring the error over training also monitor classification\n",
    "        # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "        data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "        # Use the created objects to initialise a new Optimiser instance.\n",
    "        optimiser = Optimiser(\n",
    "            model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "        # Run the optimiser for num_epochs epochs (full passes through the training set)\n",
    "        # printing statistics every epoch.\n",
    "        stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "        return stats, keys, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Width(Experiment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, widths=[32, 64, 128]):\n",
    "        stats_list = []\n",
    "\n",
    "        # Setup hyperparameters\n",
    "        learning_rate = 9e-4\n",
    "        num_epochs = 100\n",
    "        stats_interval = 1\n",
    "\n",
    "        for width in widths:\n",
    "\n",
    "            input_dim, output_dim, hidden_dim = 784, 47, width\n",
    "\n",
    "            # Create data provider objects for the MNIST data set\n",
    "            self.train_data = EMNISTDataProvider('train', batch_size=self.batch_size, rng=self.rng)\n",
    "            self.valid_data = EMNISTDataProvider('valid', batch_size=self.batch_size, rng=self.rng)\n",
    "\n",
    "            # re-init params\n",
    "            weights_init = GlorotUniformInit(rng=self.rng)\n",
    "            biases_init = ConstantInit(0.)\n",
    "\n",
    "            # Create model with ONE hidden layer\n",
    "            model = MultipleLayerModel([\n",
    "                AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # hidden layer\n",
    "                ReluLayer(),\n",
    "                AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "            ])\n",
    "\n",
    "            error = CrossEntropySoftmaxError()\n",
    "            # Use a Adam learning rule\n",
    "            learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "            print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "            print(f\"------------------------- Total Epoch: {num_epochs}, Width: {width} -------------------------\")\n",
    "\n",
    "            print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "            # Remember to use  when you write a script to be run in a terminal\n",
    "            stats, keys, run_time = self.train_model_and_plot_stats(\n",
    "                model, error, learning_rule, self.train_data, self.valid_data, num_epochs, stats_interval, notebook=True)\n",
    "            \n",
    "            stats_list.append(stats)\n",
    "\n",
    "        # Plot the change in the validation and training set error over training.\n",
    "        fig_1 = plt.figure(figsize=(8, 4))\n",
    "        ax_1 = fig_1.add_subplot(111)\n",
    "        \n",
    "        for k in ['error(train)', 'error(valid)']:\n",
    "\n",
    "            if k == 'error(train)':\n",
    "                    linestyle = \"-\"\n",
    "                    label = \"(train)\"\n",
    "            else: \n",
    "                linestyle = \"--\"\n",
    "                label = \"(valid)\"\n",
    "\n",
    "            for idx, stats in enumerate(stats_list):\n",
    "\n",
    "                print(f\"Width: {widths[idx]}, Final error{label}: {stats[-1, keys[k]]}\")\n",
    "\n",
    "                ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                        stats[1:, keys[k]], label=f\"width {widths[idx]}{label}\", linestyle=linestyle)\n",
    "                \n",
    "                \n",
    "        ax_1.legend(loc=0)\n",
    "        ax_1.set_xlabel('Epoch number')\n",
    "        ax_1.set_ylabel('Error')\n",
    "\n",
    "        plt.savefig(\"./report/figures/Task1_error_curve_width.pdf\")\n",
    "\n",
    "        # Plot the change in the validation and training set accuracy over training.\n",
    "        fig_2 = plt.figure(figsize=(8, 4))\n",
    "        ax_2 = fig_2.add_subplot(111)\n",
    "\n",
    "        for k in ['acc(train)', 'acc(valid)']:\n",
    "\n",
    "            if k == 'acc(train)':\n",
    "                    linestyle = \"-\"\n",
    "                    label = \"(train)\"\n",
    "            else: \n",
    "                linestyle = \"--\"\n",
    "                label = \"(valid)\"\n",
    "\n",
    "            for idx, stats in enumerate(stats_list):\n",
    "\n",
    "                print(f\"Width: {widths[idx]}, Final acc{label}: {stats[-1, keys[k]]}\")\n",
    "\n",
    "                ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                        stats[1:, keys[k]], label=f\"width {widths[idx]}{label}\", linestyle=linestyle)\n",
    "            \n",
    "        ax_2.legend(loc=0)\n",
    "        ax_2.set_xlabel('Epoch number')\n",
    "        ax_2.set_ylabel('Accuracy')\n",
    "\n",
    "        plt.savefig(\"./report/figures/Task1_acc_curve_width.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Depth(Experiment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, depths=[1, 2, 3]):\n",
    "        stats_list = []\n",
    "\n",
    "        # Setup hyperparameters\n",
    "        learning_rate = 9e-4\n",
    "        num_epochs = 100\n",
    "        stats_interval = 1\n",
    "        input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "        for depth in depths:\n",
    "\n",
    "            # Create data provider objects for the MNIST data set\n",
    "            self.train_data = EMNISTDataProvider('train', batch_size=self.batch_size, rng=self.rng)\n",
    "            self.valid_data = EMNISTDataProvider('valid', batch_size=self.batch_size, rng=self.rng)\n",
    "\n",
    "            weights_init = GlorotUniformInit(rng=self.rng)\n",
    "            biases_init = ConstantInit(0.)\n",
    "\n",
    "            model_list = [AffineLayer(input_dim, hidden_dim, weights_init, biases_init), ReluLayer()] # init\n",
    "            for _ in range(depth-1):\n",
    "                model_list.append(AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init))\n",
    "                model_list.append(ReluLayer())\n",
    "            model_list.append(AffineLayer(hidden_dim, output_dim, weights_init, biases_init))\n",
    "\n",
    "            print(model_list)\n",
    "\n",
    "            # Create model with TWO hidden layers\n",
    "            model = MultipleLayerModel(model_list)\n",
    "\n",
    "            error = CrossEntropySoftmaxError()\n",
    "            # Use a Adam learning rule\n",
    "            learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "            print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "            print(f\"------------------------- Total Epoch: {num_epochs}, Depth: {depth} -------------------------\")\n",
    "\n",
    "            print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "            # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "            stats, keys, run_time = self.train_model_and_plot_stats(\n",
    "                model, error, learning_rule, self.train_data, self.valid_data, num_epochs, stats_interval, notebook=True)\n",
    "            \n",
    "            stats_list.append(stats)\n",
    "\n",
    "        # Plot the change in the validation and training set error over training.\n",
    "        fig_1 = plt.figure(figsize=(8, 4))\n",
    "        ax_1 = fig_1.add_subplot(111)\n",
    "        \n",
    "        for k in ['error(train)', 'error(valid)']:\n",
    "\n",
    "            if k == 'error(train)':\n",
    "                    linestyle = \"-\"\n",
    "                    label = \"(train)\"\n",
    "            else: \n",
    "                linestyle = \"--\"\n",
    "                label = \"(valid)\"\n",
    "\n",
    "            for idx, stats in enumerate(stats_list):\n",
    "\n",
    "                print(f\"Depth: {depths[idx]}, Final error{label}: {stats[-1, keys[k]]}\")\n",
    "\n",
    "                ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                        stats[1:, keys[k]], label=f\"depth {depths[idx]}{label}\", linestyle=linestyle)\n",
    "                \n",
    "                \n",
    "        ax_1.legend(loc=0)\n",
    "        ax_1.set_xlabel('Epoch number')\n",
    "        ax_1.set_ylabel('Error')\n",
    "\n",
    "        plt.savefig(\"./report/figures/Task1_error_curve_depth.pdf\")\n",
    "\n",
    "        # Plot the change in the validation and training set accuracy over training.\n",
    "        fig_2 = plt.figure(figsize=(8, 4))\n",
    "        ax_2 = fig_2.add_subplot(111)\n",
    "\n",
    "        for k in ['acc(train)', 'acc(valid)']:\n",
    "\n",
    "            if k == 'acc(train)':\n",
    "                    linestyle = \"-\"\n",
    "                    label = \"(train)\"\n",
    "            else: \n",
    "                linestyle = \"--\"\n",
    "                label = \"(valid)\"\n",
    "\n",
    "            for idx, stats in enumerate(stats_list):\n",
    "\n",
    "                print(f\"Depth: {depths[idx]}, Final acc{label}: {stats[-1, keys[k]]}\")\n",
    "\n",
    "                ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                        stats[1:, keys[k]], label=f\"depth {depths[idx]}{label}\", linestyle=linestyle)\n",
    "            \n",
    "        ax_2.legend(loc=0)\n",
    "        ax_2.set_xlabel('Epoch number')\n",
    "        ax_2.set_ylabel('Accuracy')\n",
    "\n",
    "        plt.savefig(\"./report/figures/Task1_acc_curve_depth.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Dropout(Experiment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create data provider objects for the MNIST data set\n",
    "        self.train_data = EMNISTDataProvider('train', batch_size=self.batch_size, rng=self.rng)\n",
    "        self.valid_data = EMNISTDataProvider('valid', batch_size=self.batch_size, rng=self.rng)\n",
    "\n",
    "    def __call__(self, prob=None, lamda=None):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            stat: all results\n",
    "        \"\"\"\n",
    "        # Setup hyperparameters\n",
    "        learning_rate = 1e-4\n",
    "        num_epochs = 100\n",
    "        stats_interval = 1\n",
    "        input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "        weights_init = GlorotUniformInit(rng=self.rng)\n",
    "        biases_init = ConstantInit(0.)\n",
    "\n",
    "        model_list = [ \n",
    "            AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "            ReluLayer(),\n",
    "            DropoutLayer(rng=self.rng, incl_prob=prob),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "            ReluLayer(),\n",
    "            DropoutLayer(rng=self.rng, incl_prob=prob),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "            ReluLayer(),\n",
    "            DropoutLayer(rng=self.rng, incl_prob=prob),\n",
    "            AffineLayer(hidden_dim, output_dim, weights_init, biases_init),\n",
    "            ]\n",
    "\n",
    "        # Create model with TWO hidden layers\n",
    "        model = MultipleLayerModel(model_list)\n",
    "\n",
    "        error = CrossEntropySoftmaxError()\n",
    "        # Use a Adam learning rule\n",
    "        learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "        print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(f\"------------------------- Total Epoch: {num_epochs}, Prob: {prob}, Lambda: {lamda} -------------------------\")\n",
    "\n",
    "        print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "        stats, keys, run_time = self.train_model_and_plot_stats(\n",
    "            model, error, learning_rule, self.train_data, self.valid_data, num_epochs, stats_interval, notebook=True)\n",
    "        \n",
    "        result_path = \"./cw1/result/Dropout/\"\n",
    "\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "\n",
    "        file_path = os.path.join(result_path, \"log.txt\")\n",
    "\n",
    "        with open(file_path, \"a\") as f:\n",
    "\n",
    "            error = []\n",
    "            acc = []\n",
    "\n",
    "            for k in ['error(train)', 'error(valid)']:\n",
    "\n",
    "                print(k, stats[-1, keys[k]])    \n",
    "                error.append(stats[-1, keys[k]])\n",
    "\n",
    "            for k in ['acc(train)', 'acc(valid)']:\n",
    "                    \n",
    "                print(k, stats[-1, keys[k]])\n",
    "                acc.append(stats[-1, keys[k]])\n",
    "\n",
    "            f.write(f\"Epoch: {num_epochs}, \" + \"Dropout, \" + f\"prob = {prob}: \\n\" + f\"Validation Acc: {np.round(100 * acc[1], 1)}, \" + f\"Train Error: {np.around(error[0], 3)}, \" + f\"Validation Error: {np.around(error[1], 3)} \\n\")\n",
    "        \n",
    "        return stats, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_L1(Experiment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create data provider objects for the MNIST data set\n",
    "        self.train_data = EMNISTDataProvider('train', batch_size=self.batch_size, rng=self.rng)\n",
    "        self.valid_data = EMNISTDataProvider('valid', batch_size=self.batch_size, rng=self.rng)\n",
    "\n",
    "    def __call__(self, prob=None, lamda=None):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            stat: all results\n",
    "        \"\"\"\n",
    "        # Setup hyperparameters\n",
    "        learning_rate = 1e-4\n",
    "        num_epochs = 100\n",
    "        stats_interval = 1\n",
    "        input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "        weights_init = GlorotUniformInit(rng=self.rng)\n",
    "        biases_init = ConstantInit(0.)\n",
    "\n",
    "        model_list = [ \n",
    "            AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(coefficient=lamda), biases_penalty=L1Penalty(coefficient=lamda)), \n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(coefficient=lamda), biases_penalty=L1Penalty(coefficient=lamda)),  \n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(coefficient=lamda), biases_penalty=L1Penalty(coefficient=lamda)),  \n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty=L1Penalty(coefficient=lamda), biases_penalty=L1Penalty(coefficient=lamda)),\n",
    "            ]\n",
    "\n",
    "        # Create model with TWO hidden layers\n",
    "        model = MultipleLayerModel(model_list)\n",
    "\n",
    "        error = CrossEntropySoftmaxError()\n",
    "        # Use a Adam learning rule\n",
    "        learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "        print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(f\"------------------------- Total Epoch: {num_epochs}, Prob: {prob}, Lambda: {lamda} -------------------------\")\n",
    "\n",
    "        print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "        stats, keys, run_time = self.train_model_and_plot_stats(\n",
    "            model, error, learning_rule, self.train_data, self.valid_data, num_epochs, stats_interval, notebook=True)\n",
    "        \n",
    "        result_path = \"./cw1/result/L1/\"\n",
    "\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "\n",
    "        file_path = os.path.join(result_path, \"log.txt\")\n",
    "\n",
    "        with open(file_path, \"a\") as f:\n",
    "\n",
    "            error = []\n",
    "            acc = []\n",
    "\n",
    "            for k in ['error(train)', 'error(valid)']:\n",
    "\n",
    "                print(k, stats[-1, keys[k]])    \n",
    "                error.append(stats[-1, keys[k]])\n",
    "\n",
    "            for k in ['acc(train)', 'acc(valid)']:\n",
    "                    \n",
    "                print(k, stats[-1, keys[k]])\n",
    "                acc.append(stats[-1, keys[k]])\n",
    "\n",
    "            f.write(f\"Epoch: {num_epochs}, \" + \"Lambda1, \" + f\"lambda = {lamda}: \\n\" + f\"Validation Acc: {np.round(100 * acc[1], 1)}, \" + f\"Train Error: {np.around(error[0], 3)}, \" + f\"Validation Error: {np.around(error[1], 3)} \\n\")\n",
    "        \n",
    "        return stats, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_L2(Experiment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create data provider objects for the MNIST data set\n",
    "        self.train_data = EMNISTDataProvider('train', batch_size=self.batch_size, rng=self.rng)\n",
    "        self.valid_data = EMNISTDataProvider('valid', batch_size=self.batch_size, rng=self.rng)\n",
    "\n",
    "    def __call__(self, prob=None, lamda=None):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            stat: all results\n",
    "        \"\"\"\n",
    "        # Setup hyperparameters\n",
    "        learning_rate = 1e-4\n",
    "        num_epochs = 100\n",
    "        stats_interval = 1\n",
    "        input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "        weights_init = GlorotUniformInit(rng=self.rng)\n",
    "        biases_init = ConstantInit(0.)\n",
    "\n",
    "        model_list = [ \n",
    "            AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(coefficient=lamda), biases_penalty=L2Penalty(coefficient=lamda)), \n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(coefficient=lamda), biases_penalty=L2Penalty(coefficient=lamda)),  \n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(coefficient=lamda), biases_penalty=L2Penalty(coefficient=lamda)),  \n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty=L2Penalty(coefficient=lamda), biases_penalty=L2Penalty(coefficient=lamda))\n",
    "            ]\n",
    "\n",
    "        # Create model with TWO hidden layers\n",
    "        model = MultipleLayerModel(model_list)\n",
    "\n",
    "        error = CrossEntropySoftmaxError()\n",
    "        # Use a Adam learning rule\n",
    "        learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "        print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(f\"------------------------- Total Epoch: {num_epochs}, Prob: {prob}, Lambda: {lamda} -------------------------\")\n",
    "\n",
    "        print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "        stats, keys, run_time = self.train_model_and_plot_stats(\n",
    "            model, error, learning_rule, self.train_data, self.valid_data, num_epochs, stats_interval, notebook=True)\n",
    "        \n",
    "        result_path = \"./cw1/result/L2/\"\n",
    "\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "\n",
    "        file_path = os.path.join(result_path, \"log.txt\")\n",
    "\n",
    "        with open(file_path, \"a\") as f:\n",
    "\n",
    "            error = []\n",
    "            acc = []\n",
    "\n",
    "            for k in ['error(train)', 'error(valid)']:\n",
    "\n",
    "                print(k, stats[-1, keys[k]])    \n",
    "                error.append(stats[-1, keys[k]])\n",
    "\n",
    "            for k in ['acc(train)', 'acc(valid)']:\n",
    "                    \n",
    "                print(k, stats[-1, keys[k]])\n",
    "                acc.append(stats[-1, keys[k]])\n",
    "\n",
    "            f.write(f\"Epoch: {num_epochs}, \" + \"Lambda2, \" + f\"lambda = {lamda}: \\n\" + f\"Validation Acc: {np.round(100 * acc[1], 1)}, \" + f\"Train Error: {np.around(error[0], 3)}, \" + f\"Validation Error: {np.around(error[1], 3)} \\n\")\n",
    "        \n",
    "        return stats, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Label_Smoothing(Experiment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_data = EMNISTDataProvider('train', batch_size=self.batch_size, rng=self.rng, smooth_labels=True)\n",
    "        self.valid_data = EMNISTDataProvider('valid', batch_size=self.batch_size, rng=self.rng, smooth_labels=True)  \n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        # Setup hyperparameters\n",
    "        learning_rate = 1e-4\n",
    "        num_epochs = 100\n",
    "        stats_interval = 1\n",
    "        input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "        weights_init = GlorotUniformInit(rng=self.rng)\n",
    "        biases_init = ConstantInit(0.)\n",
    "\n",
    "        model_list = [ \n",
    "            AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, output_dim, weights_init, biases_init),\n",
    "            ]\n",
    "\n",
    "        # Create model with TWO hidden layers\n",
    "        model = MultipleLayerModel(model_list)\n",
    "\n",
    "        error = CrossEntropySoftmaxError()\n",
    "        # Use a Adam learning rule\n",
    "        learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "        print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        print(f\"------------------------- Total Epoch: {num_epochs}, Label Smoothing: {0.1} -------------------------\")\n",
    "\n",
    "        print(f\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "        stats, keys, run_time = self.train_model_and_plot_stats(\n",
    "            model, error, learning_rule, self.train_data, self.valid_data, num_epochs, stats_interval, notebook=True)\n",
    "        \n",
    "        result_path = \"./cw1/result/Label Smoothing/\"\n",
    "\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "\n",
    "        file_path = os.path.join(result_path, \"log.txt\")\n",
    "\n",
    "        with open(file_path, \"a\") as f:\n",
    "\n",
    "            error = []\n",
    "            acc = []\n",
    "\n",
    "            for k in ['error(train)', 'error(valid)']:\n",
    "\n",
    "                print(k, stats[-1, keys[k]])    \n",
    "                error.append(stats[-1, keys[k]])\n",
    "\n",
    "            for k in ['acc(train)', 'acc(valid)']:\n",
    "                    \n",
    "                print(k, stats[-1, keys[k]])\n",
    "                acc.append(stats[-1, keys[k]])\n",
    "\n",
    "            f.write(f\"Epoch: {num_epochs}, \" + \"Label Smoothing, \" + \"alpha = 0.1: \\n\" + f\"Validation Acc: {np.round(100 * acc[1], 1)}, \" + f\"Train Error: {np.around(error[0], 3)}, \" + f\"Validation Error: {np.around(error[1], 3)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Exp_Width()\n",
    "exp()\n",
    "\n",
    "exp = Exp_Depth()\n",
    "exp()\n",
    "\n",
    "exp = Exp_Dropout()\n",
    "exp(prob=0.7)\n",
    "\n",
    "exp = Exp_L1()\n",
    "exp(lamda=1e-3)\n",
    "\n",
    "exp = Exp_L2()\n",
    "exp(lamda=1e-3)\n",
    "\n",
    "exp = Exp_Label_Smoothing()\n",
    "exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
